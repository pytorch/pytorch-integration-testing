[
    {
        "test_name": "throughput_gemma3_12b_it_awq_int4_torchao",
        "parameters": {
            "model": "pytorch/gemma-3-12b-it-AWQ-INT4",
            "quantization": "torchao",
            "load_format": "auto",
            "dataset": "./ShareGPT_V3_unfiltered_cleaned_split.json",
            "num_prompts": 200,
            "backend": "vllm"
        }
    },
    {
        "test_name": "throughput_gemma3_27b_it_fp8_torchao",
        "parameters": {
            "model": "pytorch/gemma-3-27b-it-FP8",
            "quantization": "torchao",
            "load_format": "auto",
            "dataset": "./ShareGPT_V3_unfiltered_cleaned_split.json",
            "num_prompts": 200,
            "backend": "vllm"
        }
    },
    {
        "test_name": "throughput_gemma3_27b_it_int4_torchao",
        "parameters": {
            "model": "pytorch/gemma-3-27b-it-INT4",
            "quantization": "torchao",
            "load_format": "auto",
            "dataset": "./ShareGPT_V3_unfiltered_cleaned_split.json",
            "num_prompts": 200,
            "backend": "vllm"
        }
    },
    {
        "test_name": "throughput_gemma3_27b_it_awq_int4_torchao",
        "parameters": {
            "model": "pytorch/gemma-3-27b-it-AWQ-INT4",
            "quantization": "torchao",
            "load_format": "auto",
            "dataset": "./ShareGPT_V3_unfiltered_cleaned_split.json",
            "num_prompts": 200,
            "backend": "vllm"
        }
    }
]
